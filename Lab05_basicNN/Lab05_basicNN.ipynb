{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Labor: Egyszerű Neurális Hálózat implementációja\n",
    "\n",
    "A gyakorlat során az eddig tanult ismeretekre támaszkodva implementáljuk a lehető legegyszerűbb, egy rejtett réteget tartalmazó (nem deep) MLP (multi-layer preceptron) vagy FF (feed-forward) neurális hálót.\n",
    "\n",
    "## A XOR probléma\n",
    "\n",
    "A XOR vagy kizáró vagy művelete egy tipikus iskolapéldája a neurális hálók lehetőségeinek demonstrációira. Ugyan a XOR kérdés könnyen implementálható `if` elágazások segítségével, az adatokat elválasztó nem lineáris (de még csak nem is egy fügvénnyel leírható) határvonal gyakorlatilag megoldhatatlan a hagyományos statisztikai / regressziós modellek segítségével.\n",
    "\n",
    "## A perceptron\n",
    "\n",
    "Vizsgáljuk meg az eddig alkalmazott Logisztikus regressziós modell struktúráját. Ha grafikusan ábrázolni szeretnénk, az alábbi alakot kapjuk:\n",
    "\n",
    "<!--\n",
    "<center><img src=\"img/perceptron.svg\" width=\"300\"></center>\n",
    "-->\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1vC5wNfMRNwx8ZRY7h6HLDVHfaeN3ASA1\" width=\"600\"></center>\n",
    "\n",
    "\n",
    "ahol\n",
    "\n",
    "$$ s = \\sum_{i=1}^n(w_i \\cdot x_i); \\qquad \\hat{y} = a(s) $$\n",
    "\n",
    "A modellünk bemenetei az $x_1$ - $x_n$ változók, az ezekhez rendelt súlyok (modellparaméterek) a $w_1$ - $w_n$ értékek, a regressziós értéket valószínűséggé konvertáló `a()`, u.n. aktivációs függvény pedig esetünkben a _sigmoid_ függvény volt. A modell kimenete a $\\hat{y}$ valószínűség érték. Ezt a struktúrát _perceptronnak_ is szokás nevezni. Kis absztrakciós késséggel nem nehéz belátni, hogy alakjában és funkciójában nagyon hasonlít egy neuron működésére: a dendriteken beérkező elektromos ingerek súlyozott összege alapján a neuron testében (soma) adott szint elérésekor aktivációs potenciál keletkezik, amelyet a neuron az axonon keresztül továbbít a következő neuronok felé.\n",
    "\n",
    "<!--\n",
    "<center><img src=\"img/neuron.svg\" width=\"600\"></center>\n",
    "-->\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1AG4xBI_j4rBXvcM-ptJD6G2n8P63s6W_\" width=\"600\"></center>\n",
    "\n",
    "\n",
    "\n",
    "A logisztikus regressziót implementáló perceptront több rétegben alkalmazva megkapjuk a multilayer perceptron / feed forward neural network arkhitektúrát:\n",
    "<!--\n",
    "<center><img src=\"img/mlp.svg\" width=\"600\"></center>\n",
    "-->\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1ZOXMS7y5cOAmB8Mwq7NN6k1Wlo-0CYFw\" width=\"600\"></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation - predikció neurális hálóval\n",
    "\n",
    "A neurális hálók esetén a bemeneti adatainkat a bemeneti rétegen elindítva, az információ rétegről rétegre halad át a modellen, az információ a bemenetektől a kimenetig előrefele terjed, *'propagál'*. Vizsgáljuk meg, hogy hogyan fognak alakulni az egyes mátrixok méretei, miközben ez a folyamat végbemegy.\n",
    "\n",
    "A bemeneti mátrixunk az eddigiekkel megegyezően néz ki, minden sora egy-egy külön bemeneti adatpont, oszlopai tartalmazzák az egyes bementi változókat:\n",
    "\n",
    "$$\n",
    "\\underset{[m \\ \\times \\ n(0)+1]}{\\mathbf{X}} = \\left[\n",
    "\t\\begin{array}{ccccc}\n",
    " \t\t1 & x_{1,1} & x_{1,2} & \\ldots & x_{1,n(0)}\\\\\n",
    "\t\t1 & x_{2,1} & x_{2,2} & \\ldots & x_{2,n(0)}\\\\\n",
    " \t\t\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "        1 & x_{m,1} & x_{m,1} & \\ldots & x_{m,n(0)}\\\\\n",
    "\t\\end{array}\t\\right]\n",
    "$$\n",
    "ahol $n0$ a bemeneti réteg neuronjainak a száma.\n",
    "\n",
    "A súlyokat tartalamzó mátrixból eddig csak egy darab szerepelt, annak pedi egy darab oszlopa volt. Az MLP struktúra esetén minden réteghez (leszámítva a bemeneti réteget) külön mátrix tartozik. Minden rétegben, neurononként egy oszlop tartalmazza az adott súlyokat, az egyes oszlopok az adott réteg adott neuronjaihoz tartoznak:\n",
    "\n",
    "$$\n",
    "\\underset{[n(k-1)+1 \\ \\times \\ n(k)]}{\\mathbf{W}^{(k)}} = \\left[\n",
    "\t\\begin{array}{ccccc}\n",
    " \t\tw_{0,1} & w_{0,2} & \\ldots & w_{0,n(k))}\\\\\n",
    "\t\tw_{1,1} & w_{1,2} & \\ldots & w_{1,n(k)}\\\\\n",
    " \t\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "        w_{n(k-1),1} & w_{n(k-1),2} & \\ldots & w_{n(k-1),n(k)}\\\\\n",
    "\t\\end{array}\\right]\n",
    "$$\n",
    "ahol $n(k)$ az n. rétegben található neuronok száma, $n(k-1)$ pedig az azt megelőző réteg neuronjainak száma.\n",
    "\n",
    "A $k$ réteg kimeneteit a már ismert logisztikus regressziónak megfelelő sémával számolhatjuk:\n",
    "$$\\underset{[m \\ \\times \\ n(k)]}{\\mathbf{\\hat{Y}}^{(k)}} = a\\left(\\underset{[m \\ \\times \\ n(k)]}{\\mathbf{S}^{(k)}}\\right) = a\\left(\\underset{[m \\ \\times \\ n(k-1)+1]}{\\mathbf{X}^{(k)}} \\cdot \\underset{[n(k-1)+1 \\ \\times \\ n(k)]}{\\mathbf{W}^{(k)}} \\right)$$\n",
    "$$\\underset{[m \\ \\times \\ n(k-1)+1]}{\\mathbf{X}^{(k)}} = \\underset{[m \\ \\times \\ 1]}{BIAS} + \\underset{[m \\ \\times \\ n(k-1)]}{\\mathbf{Y}^{(k-1)}}$$\n",
    "ahol $a$ az hálóban (vagy adott rétegben) haszált aktivációs függvény. A tárgy keretében csak a sigmoid függvénnyel foglalkoztunk, mint aktivációs függvény, de számos másik létezik, amelyek közül szinte akármelyik szabadon alkalmazható. Az egyetlen megkötés általában az utolós rétegnél van, ahol a költségfüggvényhez hasonlóan egyeztetni kell az aktivációs függvényt a feladattal, hiszen ez határozza meg, hogy  a kimenet milyen tartományú lehet.:\n",
    "- regresszió --> MSE & identitás aktivációs függvény\n",
    "- bináris klasszifikáció --> BCE & sigmoid aktivációs függvény\n",
    "\n",
    "Egy egyszerű, 2 bemeneti neuront és egy  rejtett rétegben 3 neuront tartalmazó bináris klasszifikációt megvalósító háló esetén a mátrixok méretének alakulása a forward propagation folyamán:\n",
    "- a rejtett rétegre\n",
    "$$ \\underset{m \\ \\times \\ 3}{\\mathbf{X^{(1)}}} =  \\underset{m \\ \\times \\ 1}{BIAS} + \\underset{m \\ \\times \\ 2}{\\mathbf{X}} $$\n",
    "$$ \\underset{m \\ \\times \\ 3}{\\mathbf{S^{(1)}}} = \\underset{m \\times 3}{\\mathbf{X^{(1)}}} \\times \\underset{ 3 \\ \\times \\ 3}{\\mathbf{W^{(1)}}} $$\n",
    "$$ \\underset{m \\ \\times \\ 3}{\\mathbf{\\hat{Y}}^{(1)}} = sigmoid(\\underset{m \\ \\times \\ 3}{{\\mathbf{S}^{(1)}}}) $$\n",
    "- a kimeneti rétegre\n",
    "$$ \\underset{m \\times 4}{\\mathbf{X^{(2)}}} = \\underset{[m \\ \\times \\ 1]}{BIAS} + \\underset{m \\ \\times \\ 3}{\\mathbf{\\hat{Y}}^{(1)}} $$\n",
    "$$ \\underset{m \\ \\times \\ 1}{\\mathbf{S^{(2)}}} = \\underset{m \\times 4}{\\mathbf{X^{(2)}}} \\times \\underset{ 4 \\ \\times \\ 1}{\\mathbf{W^{(2)}}} $$\n",
    "$$ \\underset{m \\ \\times \\ 1}{\\mathbf{\\hat{Y}}^{(2)}} = sigmoid(\\underset{m \\ \\times \\ 1}{{\\mathbf{S}^{(2)}}}) $$\n",
    "Az utolsó rétegünk kimenete maga a modell által adott becslés.\n",
    "$$ \\underset{m \\ \\times \\ 1}{\\mathbf{\\hat{Y}}} = \\underset{m \\ \\times \\ 1}{\\mathbf{\\hat{Y}}^{(2)}} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackPropagation - neurális háló tanítása\n",
    "\n",
    "A neurális hálók esetén a gradiens módszer elve nem változik. Az egyes súlyokat a költségfüggvény adott súly szerinti parciális deriváltjának alapján módosítjuk, azonban a gradiens számítása némileg komplikáltabb. Számítására a lánc szabályt használhatjuk. Nézzük meg a gradiens számítását a kimeneti réteghez tartozó súlyokra. A kimeneti réteg súlyaihoz tartozó parciális derivált a láncszabály alapján általánosan, $p$ az utolsó (kimeneti) réteg:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w^{(p)}_{ij}} =\n",
    "\\frac{\\partial C}{\\partial \\hat  y_j} \\frac{\\partial \\hat y_j}{\\partial w^{(p)}_{ij}} = \n",
    "\\frac{\\partial C}{\\partial \\hat  y_j} \\frac{\\partial \\hat  y_j}{\\partial s^{(p)}_j} \\frac{\\partial s^{(p)}_j}{\\partial w^{(p)}_{ij}} $$\n",
    "\n",
    "Az előzőekben defináltuk, hogy\n",
    "\n",
    "$$  \\hat  y_j = \\hat y^{(p)}_{j}; \\qquad \\hat y^{(k)}_{j}=a(s^{(k)}_j); \\qquad s^{(k)}_j=\\sum_{i=1}^n x^{(k)}_i w^{(k)}_{ij}; $$\n",
    "\n",
    "amiből\n",
    "$$ \\frac{\\partial \\hat  y_j}{\\partial s^{(p)}_j} = a'(s^{(p)}_j); \\qquad \\frac{\\partial s^{(p)}_j}{\\partial w^{(p)}_{ij}} = x^{(p)}_i$$\n",
    "\n",
    "Ezek alapján az utolsó réteghez tartozó parciális deriváltak:\n",
    "$$ \\frac{\\partial C}{\\partial w^{(p)}_{ij}} =  \\frac{\\partial C}{\\partial \\hat  y_j} a'(s^{(p)}_j) x^{(p)}_i = \\delta^{(p)}_j x^{(p)}_i; \\qquad  \\delta^{(p)}_j = \\frac{\\partial C}{\\partial \\hat  y_j} a'(s^{(p)}_j) $$\n",
    "\n",
    "A több bemenetet figyelembe véve és mátrixos alakra felírva:\n",
    "\n",
    "$$ \\boxed{ \\frac{\\partial C}{\\partial \\mathbf{W}^{(p)}} =  \\left(\\mathbf{X^{(p)}_i}\\right)^T \\left( \\frac{\\partial C}{\\partial \\mathbf{\\hat  Y}} a'(\\mathbf{S}^{(p)}) \\right)  = \\left(\\mathbf x^{(p)}\\right)^T \\cdot \\mathbf \\delta^{(p)} } $$\n",
    "\n",
    "\n",
    "A *BCE* költségfüggvény és a *sigmoid* aktiváció együttes használatakor\n",
    "$$ \\boxed{\\mathbf \\delta^{(p)} = \\frac{1}{m} \\left(\\mathbf{\\hat Y} - \\mathbf{Y} \\right) = \\frac{1}{m} \\left(sigmoid(\\mathbf{X}^{(p)}\\cdot \\mathbf{W}^{(p)} ) - \\mathbf{Y} \\right)}$$\n",
    "\n",
    "Látható, hogy visszakaptuk az egyzserű logisztikus regresszió esetén alkalmazott gradiensvektort. A $k$ rejtett réteg esetében a gradiensek számítása a következőként írható fel:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w^{(k)}_{ij}} =\n",
    "\\frac{\\partial C}{\\partial y^{(k)}_{j}} \\frac{\\partial y^{(k)}_{j}}{\\partial w^{(k)}_{ij}} =\n",
    "\\frac{\\partial C}{\\partial y^{(k)}_{j}} \\frac{\\partial y^{(k)}_{j}}{\\partial s^{(k)}_{j}}\\frac{\\partial s^{(k)}_{j}}{\\partial w^{(k)}_{ij}} =\n",
    "\\frac{\\partial C}{\\partial y^{(k)}_j} a'(s^{(k)}_j) x^{(k)}_i=\n",
    "\\delta^{(k)}_j x^{(k)}_i; $$\n",
    "\n",
    "Mátrixos formában:\n",
    "$$ \\boxed{ \\frac{\\partial C}{\\partial \\mathbf{W}^{(k)}} = (\\mathbf{X}^{(k)})^T \\cdot \\mathbf{\\delta^{(k)}} }$$\n",
    "\n",
    "A $k$ réteghez tartozó delta tag számítása pedig\n",
    "\n",
    "$$ \\mathbf{\\delta^{(k)}} = \\frac{\\partial C}{\\partial \\mathbf{\\hat  Y^{(k)}}} a'(\\mathbf{s^{(k)}}) = \\frac{\\partial C}{\\partial \\mathbf{\\hat  Y^{(k+1)}}} \\frac{\\partial \\mathbf{\\hat  Y^{(k+1)}}}{\\partial \\mathbf{\\hat  Y^{(k)}}}a'(\\mathbf{S^{(k)}}) = \\frac{\\partial C}{\\partial \\mathbf{\\hat  Y^{(k+1)}}} \\frac{\\partial \\mathbf{\\hat  Y^{(k+1)}}}{\\partial \\mathbf{\\hat  S^{(k+1)}}} \\frac{\\partial \\mathbf{ S^{(k+1)}}}{\\partial \\mathbf{\\hat  Yy^{(k)}}}a'(\\mathbf{S^{(k)}})$$\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{\\hat  Y^{(k+1)}}}{\\partial \\mathbf{ S^{(k+1)}}} = a'(\\mathbf{S^{(k+1)}})$$\n",
    "$$\\frac{\\partial \\mathbf{ S^{(k+1)}}}{\\partial \\mathbf{\\hat  Y^{(k)}}} = \\frac{\\partial \\mathbf{ S^{(k+1)}}}{\\partial \\mathbf{\\hat  X^{(k+1)}}} = (\\mathbf{W}^{(k+1)})^T$$\n",
    "\n",
    "$$\\boxed{ \\mathbf{\\delta^{(k)}} = \\frac{\\partial C}{\\partial \\mathbf{\\hat  Y^{(k+1)}}} a'(\\mathbf{S^{(k+1)}}) (\\mathbf{W}^{(k+1)})^T a'(\\mathbf{S^{(k)}}) = \\left( \\mathbf \\delta^{(i+1)} \\cdot \\left( \\mathbf W^{(i+1)} \\right)^T \\right) a'\\left(\\mathbf s^{(i)} \\right) }$$\n",
    "\n",
    "Látszik, hogy az implementációhoz mindenképp szükésg van az aktivációs függvény deriváltjára is. A sigmoid függvény esetében:\n",
    "$$ \\boxed{ sigmoid'(z) = sigmoid(z)(1-sigmoid(z))} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00: Könyvtár importálások\n",
    "\n",
    "Első lépésként importáljuk a feladat megoldása során használt könyvtárakat. Esetünkben ezek a következők lesznek:\n",
    "- Numpy a matematikia műveletek elvégzéséhez\n",
    "- Pandas az adatok beolvasásához és kezeléséhez\n",
    "- MatPlotLib.pyplot az eredményeink ábrázolásához\n",
    "- Plotly Express interaktív vizualizációhoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Használjuk ezeket sötét téma esetén\n",
    "plt.style.use('dark_background')\n",
    "styleTemplate = 'plotly_dark'\n",
    "\n",
    "# Használjuk ezeket világos téma esetén\n",
    "#plt.style.use('default')\n",
    "#styleTemplate = 'plotly_white'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01: Adatgenerálás\n",
    "Az eddigiekkel eltérő módon most nem egy előre adott adatokkal dolgozunk. A XOR probléma adatstruktúrája viszonylag egyszerű, az adatokat magunknak generálhatjuk. Így tetszőleges számú adatot használhatunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE XOR DATA\n",
    "nSamples = 100 # total samples = 4*nSamples\n",
    "clusters = [[-0.5, -0.5, 0], [-0.5, 0.5, 1], [0.5, -0.5, 1], [0.5, 0.5, 0]]\n",
    "std = 0.25\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "X = np.ones([4*nSamples, 2])\n",
    "Y = np.ones([4*nSamples, 1])\n",
    "for count, params in enumerate(clusters):\n",
    "    X[count*nSamples:(count+1)*nSamples, 0] = rng.normal(params[0], std, nSamples)\n",
    "    X[count*nSamples:(count+1)*nSamples:,1] = rng.normal(params[1], std, nSamples)\n",
    "    Y[count*nSamples:(count+1)*nSamples] = params[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02: Adatfelfedezés\n",
    "\n",
    "A szintetikus, generált adatok esetén is érdemes előzetes adatfelfedezést / adatvizualizációt alkalmazni. Így ellenőrizni tudjuk, hogy biztodan oylan adatokat generáltunk-e, mint szerettünk volna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "falseData = X[Y[:,0] == 0, :]\n",
    "trueData = X[Y[:,0] == 1, :]\n",
    "\n",
    "plt.scatter(falseData[:, 0], falseData[:, 1], marker='o', c=\"r\", label=\"False\")\n",
    "plt.scatter(trueData[:, 0], trueData[:, 1], marker='o', c=\"g\", label=\"True\")\n",
    "\n",
    "plt.title(\"Generált XOR data\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generált adataink ebben az esetben már megfelelnek a tanításra, a tartományukból adódóan normalizálásra nincs szükség. A bias tag implementációját most a modellen belül oldjuk meg."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Modell implementálása\n",
    "\n",
    "Először implementáljuk az aktivációs függvényként használt `sigmoid()` függvényt. Mivel a hálónk tanításához szükségünk lessz a sigmoid deriváltjának számítására is, az implementációnkba ezt is belefoglaljuk."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feladat:** implementálja a `sigmoid()` aktivációs függvényt, amely második bementeként egy `bool` értéket vár, ami alapján vagy a *sigmoid* függvényt vagy annak a deriváltját számítja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z, derivate = False):       # Alapértelmezett a rendes sigmoid érték számítása\n",
    "######################################\n",
    "\n",
    "######################################\n",
    "    return g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az implementációnkat most vizuálisan ellenőrizzük."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 100)\n",
    "y = sigmoid(x)\n",
    "dy = sigmoid(x, derivate = True)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = x, y = y, mode='lines', name ='sigmoid'))\n",
    "fig.add_trace(go.Scatter(x = x, y = dy, mode='lines', name = 'derivate of sigmoid'))\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',\n",
    "    xaxis_title = \"z\",\n",
    "    title = \"Sigmoid függvény\",\n",
    "    width=600,\n",
    "    height=320,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y = 1.02,\n",
    "        xanchor=\"center\",\n",
    "        x = 0.5)\n",
    ")\n",
    "\n",
    "fig.update_traces(line=dict( width=3))\n",
    "fig.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='grey', zeroline=True, zerolinewidth=3, zerolinecolor='grey')\n",
    "fig.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='grey', zeroline=True, zerolinewidth=3, zerolinecolor='grey')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Szintén külön függvényként definiáljuk a bemeneti mátrix biassal való kiegészítését."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addBias(z):\n",
    "    return np.hstack([np.ones([z.shape[0] ,1]), z])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias kiegészítés tesztelése:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = np.array([[0.3146, -0.65432, 0.24], [-1.0123, -0.4215, -0.12412], [0.2351, 0.7533456, 2.346]])\n",
    "print(addBias(testX))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neurális háló implementálásához és tanításához szükséges metódusokat és adattagokat most egy osztályba rendezve implementáljuk. A még nem üres metódusokat egy `pass` utasítással feltöltve elkerülhetjük, hogy a futtatás során hibát jelezzen, így egyenként ellenőrizhetjük a metódusainkat.\n",
    "\n",
    "**Feladat:** implementálja a háló `forwardProp()` metódusát, amely elvégzi az adott bemeneti adatokra a előreterjesztés lépését! A metódus töltse fel az osztály belső X[k] és Yhat[k] listákat a megfelelő értékekkel! Az implementáció ellenőrzésére használja a forwardProp tesztelő cellát az osztálydefiníció alatt.\n",
    "\n",
    "**Feladat:** implementálja a háló `backProp()` metódusát, amely a backPropagation módzser segítségével kiszámolja az egyes súlyokhoz tartozó gradienseket. Az implementáció ellenőrzésére használja a backProp tesztelő cellát az osztálydefiníció alatt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self, layerSizes, activationFunction, seed):\n",
    "        self.layerSizes = layerSizes            # Háló konfigurációja\n",
    "        self.a = activationFunction             # Aktivációs függvény (később más aktivációs függvény is alkalmazható)\n",
    "        self.noLayers = len(layerSizes)-1       # Rétegek száma (iterációhoz, bemeneti réteg nélkül)\n",
    "        self.inputSize = layerSizes[0]          # Bemeneti változók (input feature) száma\n",
    "        self.Yhat = []                             # Lista az egyes rétegek kimenetének tárolására\n",
    "        self.X = []                             # Lista az egyes rétegek bemenetének tárolására\n",
    "        self.W = []                             # Lista a rétegekhez tartozó súlyoknak\n",
    "        self.initWeights(seed)                  # Súlyok inicializálása\n",
    "    \n",
    "    def initWeights(self, seed = None):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        # Weight matrix dimension are based on number of neurons in previous and current layer\n",
    "        self.W = [rng.uniform(low = -1, high = 1, size = [self.layerSizes[i]+1, self.layerSizes[i+1]]) for i in range(self.noLayers)]\n",
    "\n",
    "    def checkInputSize(self, X):\n",
    "        if X.shape[1] != self.inputSize:\n",
    "            raise ValueError('Unexpected number of input features! Expected {} features but got {}.'.format(self.inputSize, X.shape[1])) \n",
    "\n",
    "    def forwardProp(self, X):\n",
    "        self.X = [[] for i in range(self.noLayers)]\n",
    "        self.Yhat = [[] for i in range(self.noLayers)]\n",
    "    ######################################\n",
    "        pass\n",
    "    ######################################\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.checkInputSize(X)\n",
    "        self.forwardProp(X)\n",
    "        return self.Yhat[-1]\n",
    "\n",
    "    def costBCE(self, X, Y):\n",
    "        eps = 10e-15\n",
    "        self.forwardProp(X)\n",
    "        return np.mean(-Y*np.log(self.Yhat[-1]+eps)-(1-Y)*np.log(1-self.Yhat[-1]))\n",
    "\n",
    "    def backProp(self, trueY):\n",
    "    ######################################\n",
    "        pass\n",
    "    ######################################\n",
    "        #return dW\n",
    "\n",
    "    def updateWeights(self, learning_rate, dW):\n",
    "        for i in range(self.noLayers):\n",
    "            self.W[i] = self.W[i] - learning_rate * dW[i]\n",
    "\n",
    "    def fit(self, X, Y, learning_rate, epochs):\n",
    "        C_history = np.zeros([epochs+1])\n",
    "        C_history[0] = self.costBCE(X, Y)    # Forwardprop megtörténik\n",
    "        print('''\n",
    "        \\%\\%\\% ------- TANÍTÁS ------- \\%\\%\\%\n",
    "        ''')\n",
    "        for i in range(epochs):\n",
    "            dW = self.backProp(Y)\n",
    "            self.updateWeights(learning_rate, dW)\n",
    "            C_history[i+1] = self.costBCE(X,Y) # Forwardprop megtörténik\n",
    "\n",
    "            if ((i+1) % 250) == 0:\n",
    "                \n",
    "                print('Epoch {} / {} completed. Cost value:{}'.format(i+1, epochs, C_history[i+1])) \n",
    "\n",
    "        return C_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teszteljük az egyes metódusokat, hogy megbizonyosodjunk az elvárt működésről!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerSizes = [2, 3, 1]\n",
    "seed = 42\n",
    "testNN = FeedForwardNN(layerSizes, sigmoid, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test initweights\n",
    "print('''Expected initial weights with layer sizes {0} and random seed {1}:\n",
    "[array([[ 0.5479121 , -0.12224312,  0.71719584],\n",
    "       [ 0.39473606, -0.8116453 ,  0.9512447 ],\n",
    "       [ 0.5222794 ,  0.57212861, -0.74377273]]), array([[-0.09922812],\n",
    "       [-0.25840395],\n",
    "       [ 0.85352998],\n",
    "       [ 0.28773024]])]'''.format(layerSizes, seed))\n",
    "print('''Actual initial weights with layer sizes {0} and random seed {1}:\n",
    "{2}'''.format(layerSizes, seed, testNN.W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test checkInputSize\n",
    "testX = np.array([[0.3146, -0.65432, 0.24], [-1.0123, -0.4215, -0.12412], [0.2351, 0.7533456, 2.346]])\n",
    "try:\n",
    "    testNN.checkInputSize(testX)\n",
    "except ValueError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forwardProp & Predict\n",
    "testX = np.array([[0.3146, -0.65432], [-1.0123, -0.4215], [0.2351, 0.7533456]])\n",
    "testPred = testNN.predict(testX)\n",
    "print('''Expected prediction for test parameters:\n",
    "[[0.56445555]\n",
    " [0.610119  ]\n",
    " [0.58247854]]'''.format(layerSizes, seed))\n",
    "print('''Actual prediction for test parameters:\n",
    "{0}'''.format(testPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cost function\n",
    "testX = np.array([[0.3146, -0.65432], [-1.0123, -0.4215], [0.2351, 0.7533456]])\n",
    "testY = np.array([[0], [1], [1]])\n",
    "\n",
    "testCost = testNN.costBCE(testX, testY)\n",
    "\n",
    "print('''Expected cost value for test parameters:\n",
    "0.6219075363927603''')\n",
    "\n",
    "print('''Actual prediction for test parameters:\n",
    "{0}'''.format(testCost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test backprop\n",
    "testX = np.array([[0.3146, -0.65432], [-1.0123, -0.4215], [0.2351, 0.7533456]])\n",
    "testY = np.array([[0], [1], [1]])\n",
    "\n",
    "testNN.forwardProp(testX)\n",
    "testdW = testNN.backProp(testY)\n",
    "\n",
    "print('''Expected cost value for test parameters:\n",
    "[array([[ 0.00351405, -0.02095111, -0.01093665],\n",
    "       [-0.0105734 ,  0.03069215,  0.00971723],\n",
    "       [ 0.00944793, -0.034079  , -0.0086122 ]]), array([[-0.0809823 ],\n",
    "       [-0.05584407],\n",
    "       [-0.09301554],\n",
    "       [ 0.00406624]])]''')\n",
    "\n",
    "print('''Actual prediction for test parameters:\n",
    "{0}'''.format(testdW))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04: Modell tanítása\n",
    "\n",
    "Amennyiben minden tesztünkön megfelelő eredményt kaptunk, végezzük el a háló tanítását a `.fit()` metódus segítségével, és ábrázoljuk a költségfüggvény alakulását."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "trainNN = FeedForwardNN([2, 10, 1], sigmoid, 42)\n",
    "\n",
    "C_history = trainNN.fit(X, Y, learning_rate, 3000)\n",
    "plt.plot(range(C_history.size), C_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05: Modell értékelése"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A beépített `predict()` függvénynek köszönhetően a vizualizálhatjuk az hálónk döntési stratégiáját, vizualizálhatjuk a működést 2D-ben contúrvonalakkal, vagy 3D-ben a teljes illesztett felületet ábrázolva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "falseData = X[Y[:,0] == 0, :]\n",
    "trueData = X[Y[:,0] == 1, :]\n",
    "\n",
    "plt.scatter(falseData[:, 0], falseData[:, 1], marker='o', c=\"r\", label=\"False\")\n",
    "plt.scatter(trueData[:, 0], trueData[:, 1], marker='o', c=\"g\", label=\"True\")\n",
    "\n",
    "x1 = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 200)    # grid létrehozása\n",
    "x2 = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 200)    # második paraméter\n",
    "\n",
    "z=np.zeros((len(x1),len(x2)))                          # eredményváltozó 1 inicializálása\n",
    "\n",
    "for i in range(len(x1)):                                 # valószínűség számolása a teljes háló felett\n",
    "    for j in range(len(x2)):     \n",
    "        testPoint = np.array([[x1[i], x2[j]]])\n",
    "        z[i,j] = trainNN.predict(testPoint)\n",
    "\n",
    "plt.contour(x1, x2,z.transpose(), 3)                                  # kirajzoljuk contour plottal a döntési határt                                # kirajzoljuk contour plottal a döntési határt\n",
    "\n",
    "\n",
    "plt.title(\"XOR becslés\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A beépített `predict()` fügvénynek köszönhetően a teljes illesztett felületet is vizualizálhatjuk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ábrázolás Plotly-val\n",
    "fig = go.Figure()\n",
    "\n",
    "# A magyarázott változót transzponálni kell a helyes megjelenítésért.\n",
    "fig.add_trace(go.Scatter3d(x=X[:,0], y=X[:,1], z=Y[:,0], mode= \"markers\"))\n",
    "fig.add_trace(go.Surface(x=x1, y=x2, z=z.T, colorscale ='Blues'))\n",
    "\n",
    "#Plot formázása\n",
    "fig.update_layout(\n",
    "    title = \"XOR becslés\",\n",
    "    scene = dict(\n",
    "        xaxis_title = \"x1\",\n",
    "        yaxis_title = \"x2\",\n",
    "        zaxis_title = \"Prob\"),\n",
    "    template=styleTemplate,\n",
    "    width=750,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "#Plot megjelenítése\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06: További tesztelés\n",
    "\n",
    "A gyakorlat keretében a tesztelést csak az illesztett felület vizualizásársa, illetve a költség alakulására korlátoztuk. Az eddig tanult egyéb ismeretek alapján érdemes egyéni munkában a hálót módosítani, kiegészítgetni, pl.: accuracy számolás implementálása és nyomonkövetése a tanulás során, animáció készítés a döntési határ alakulásáról a tanulás során, vagy a regularizásciós technikák implementálása és hatásuknak vizsgálata."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
